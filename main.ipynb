{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd81b8e-1f52-48ec-93b2-a4c5e2793f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1105/3561027592.py:5: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74066d4d-e655-4128-ac74-0e80def860e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.4: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8d41c6d176448a8be9c5460c7cb334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502f4ddcef9148c49298b1c90c3062f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e8820bb881446bb3e51a9e6ed31a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2df500391fe44348d2a0e2aff2a09bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10cce50299e54189b5fca72b9c92cbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c96bbcaa0af46e0baf60d8a564e119e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/3.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_adapter_path = \"./outputs/sft_lora_adapter_generation\"\n",
    "\n",
    "g_model, g_tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "            max_seq_length=1024,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "peft_g_model = PeftModel.from_pretrained(g_model, sft_adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1fb394-7177-4aef-bf25-d336f973fa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "          \"text-generation\",\n",
    "          model=peft_g_model,\n",
    "          tokenizer=g_tokenizer,\n",
    "          max_new_tokens=500,\n",
    "          return_full_text=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b9ec53-7c41-42c9-81d3-61f5859de19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.4: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "sft_adapter_path2 = \"./outputs/sft_lora_adapter_summarization\"\n",
    "\n",
    "s_model, s_tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "            max_seq_length=1024,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "peft_s_model = PeftModel.from_pretrained(s_model, sft_adapter_path)\n",
    "\n",
    "\n",
    "pipe2 = pipeline(\n",
    "          \"text-generation\",\n",
    "          model=peft_s_model,\n",
    "          tokenizer=s_tokenizer,\n",
    "          max_new_tokens=500,\n",
    "          return_full_text=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ca2d70-f282-4042-a2cd-e3da60f42fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.4: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "sft_adapter_path3 = \"./outputs/sft_lora_adapter_evaluation\"\n",
    "\n",
    "e_model, e_tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "            max_seq_length=1024,\n",
    "            load_in_4bit=True,\n",
    "            dtype=None,\n",
    "        )\n",
    "peft_e_model = PeftModel.from_pretrained(e_model, sft_adapter_path3)\n",
    "\n",
    "\n",
    "pipe3 = pipeline(\n",
    "          \"text-generation\",\n",
    "          model=peft_e_model,\n",
    "          tokenizer=e_tokenizer,\n",
    "          max_new_tokens=500,\n",
    "          return_full_text=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978719f3-df08-4f9e-8d35-64ca000b1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb81cecc-268e-412b-b2b8-c44b5ad27e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0582e477-38e8-4559-aab5-efaa44e30488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_id(gene):\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    params = {\n",
    "        \"query\": f\"gene_exact:{gene} AND organism_id:9606\",\n",
    "        \"fields\": \"accession,gene_primary,organism_name\",\n",
    "        \"format\": \"json\",\n",
    "        \"size\": 1\n",
    "    }\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "\n",
    "    if data.get(\"results\"):\n",
    "        result = data[\"results\"][0]\n",
    "        return result[\"primaryAccession\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_gene(gene_name):\n",
    "    uniprot_id = get_uniprot_id(gene_name)\n",
    "    r = requests.get(f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}\", headers={\"Accept\": \"application/json\"})\n",
    "    data = r.json()\n",
    "\n",
    "    return data\n",
    "\n",
    "def construct_context(markers):\n",
    "    context_rag = \"This is the context that you should consider to identify the cell type\\n\\n\"\n",
    "    for i in range(len(markers)):\n",
    "        curGene = markers[i]\n",
    "        context_rag += f\"Marker gene {i+1}: {curGene}\\n\"\n",
    "        metaData = search_gene(curGene)\n",
    "        if metaData == None:\n",
    "            context_rag += \"\\n\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            protein_description = metaData['proteinDescription']\n",
    "            protein_comments = metaData['comments']\n",
    "        except Exception as e:\n",
    "            context_rag += \"\\n\"\n",
    "            continue\n",
    "\n",
    "        if \"recommendedName\" in protein_description:\n",
    "            context_rag += f\"This gene is {protein_description['recommendedName']['fullName']['value']}\\n\"\n",
    "        else:\n",
    "            context_rag += f\"This gene is {protein_description['submissionNames'][0]['fullName']['value']}\\n\"\n",
    "\n",
    "        for j in range(len(protein_comments)):\n",
    "            if j > 10:\n",
    "                break\n",
    "            curPos = protein_comments[j]\n",
    "            if \"texts\" in curPos:\n",
    "                context_rag += f\"{curPos['texts'][0]['value']}\\n\"\n",
    "\n",
    "        context_rag += \"\\n\"\n",
    "\n",
    "    return context_rag\n",
    "\n",
    "def SuRe_Generate(llm, tokenizer, markers, context_rag):\n",
    "    input_prompt = f\"\"\"You are an intelligent expert to identify the most appropriate cell type from the given marker genes.\n",
    "    You will be given marker genes and context that you should consider.\n",
    "\n",
    "    **Do NOT explain your answer. You must only return a single cell type and then finish your answer.**\n",
    "\n",
    "    Given the expression of genes {markers}, identify the most appropriate cell type.\n",
    "\n",
    "    ### Context:\n",
    "    {context_rag}\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result1 = llm(input_prompt, temperature=0.9, no_repeat_ngram_size=2, return_full_text=False)[0]['generated_text']\n",
    "    \n",
    "\n",
    "    result2 = llm(input_prompt, temperature=0.9, no_repeat_ngram_size=2, return_full_text=False)[0]['generated_text']\n",
    "    return result1, result2\n",
    "\n",
    "\n",
    "def SuRe_Summarize(llm, tokenizer, markers, context_rag, ans1, ans2):\n",
    "    answers = [ans1, ans2]\n",
    "    summaries = [\"\", \"\"]\n",
    "    for i in range(2):\n",
    "        summary_prompt = f\"\"\"You are an intelligent summarizer to check whether the given answer is proper for the given question.\n",
    "    \n",
    "    You should produce a summary that shows how well the given answer aligns with the given supporting text.\n",
    "\n",
    "\n",
    "    ### Question:\n",
    "    Given the expression of genes {markers}, identify the most likely cell type.\n",
    "\n",
    "    ### Answer:\n",
    "\n",
    "    {answers[i]}\n",
    "\n",
    "    ### Supporting Text:\n",
    "\n",
    "    {context_rag}\n",
    "\n",
    "    ### Your output should:\n",
    "\n",
    "    1. Explain which parts of the given supporting text the answer draws on.\n",
    "    2. Never be too long\n",
    "    \"\"\"\n",
    "\n",
    "        summary = llm(summary_prompt, temperature=0.9, return_full_text=False)[0]['generated_text']\n",
    "        summaries[i] = summary\n",
    "\n",
    "    return summaries[0], summaries[1]\n",
    "\n",
    "def get_score(response):\n",
    "    matches = re.search(r'score(?: is|:)\\s+[0-9]', response, re.IGNORECASE)\n",
    "    if matches == None:\n",
    "        return None\n",
    "\n",
    "    return matches.group(0)[6:].strip()\n",
    "\n",
    "\n",
    "def SuRe_Validation(eval_llm, tokenizer, summary1, summary2, markers, ans1, ans2):\n",
    "    answers = [ans1, ans2]\n",
    "    summaries = [summary1, summary2]\n",
    "    scores = [0, 0]\n",
    "\n",
    "    for i in range(2):\n",
    "        eval_prompt = f\"\"\"You are an impartial judge evaluating the answer for the given question.\n",
    "\n",
    "        Your task:\n",
    "\n",
    "        1. Read the given question, answer, and its justification.\n",
    "        2. Assign the consistency score from 0 to 10 (0 = not align at all, 10 = fully supported) based on how accurately the answer reflects the evidence.\n",
    "\n",
    "        ** Note that you must not explain your answer **\n",
    "        ** RETURN a only single number **\n",
    "\n",
    "        ### Question:\n",
    "        \n",
    "        Given the expression of genes {markers}, identify the most likely cell type.\n",
    "\n",
    "        ### Answer:\n",
    "\n",
    "        {answers[i]}\n",
    "\n",
    "        ### Justification:\n",
    "\n",
    "        {summaries[i]}\n",
    "\n",
    "        score: \n",
    "        \"\"\"\n",
    "\n",
    "        count = 0\n",
    "        while (True):\n",
    "            score = eval_llm(eval_prompt, temperature=0.9, return_full_text=False)[0]['generated_text']\n",
    "            score = score[len(eval_prompt):].strip()\n",
    "            score = get_score(score)\n",
    "      \n",
    "            try:\n",
    "                score = int(score)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if count < 10:\n",
    "                    count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    score = 0\n",
    "                    break\n",
    "        scores[i] = score\n",
    "    index = max(range(len(scores)), key=lambda i: scores[i])\n",
    "\n",
    "    return answers[index]\n",
    "\n",
    "def extract_answer(response):\n",
    "    matches = re.search(r'answer(?: is|:)\\s+(.+?)(?=[.,;!?]|\\n|[.]$)', response, re.IGNORECASE)\n",
    "\n",
    "\n",
    "    if matches == None:\n",
    "        return None\n",
    "\n",
    "    if matches.group(0)[6] == \":\":\n",
    "        return matches.group(0)[7:].strip()\n",
    "    else:\n",
    "        return matches.group(0)[9:].strip()\n",
    "\n",
    "def generate_response(llm, tokenizer, s_llm, tokenizer2, eval_llm, tokenizer3, markers):\n",
    "    markers_gene = markers.split(',')\n",
    "    context_rag = construct_context(markers_gene)\n",
    "    fin_cellType = \"\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        result1, result2 = SuRe_Generate(llm, tokenizer, markers, context_rag)\n",
    "\n",
    "        result1 = extract_answer(result1)\n",
    "        result2 = extract_answer(result2)\n",
    "\n",
    "        if result1 == None and result2 == None:\n",
    "            if count > 5:\n",
    "                return \"Sorry, I don't know\"\n",
    "            else:\n",
    "                count += 1\n",
    "                continue\n",
    "        elif result1 == None:\n",
    "            return result2\n",
    "        elif result2 == None:\n",
    "            return result1\n",
    "        else:\n",
    "            summary1, summary2 = SuRe_Summarize(s_llm, tokenizer2, markers, context_rag, result1, result2)\n",
    "            fin_cellType = SuRe_Validation(eval_llm, tokenizer3, summary1, summary2, markers, result1, result2)\n",
    "            break\n",
    "\n",
    "\n",
    "    return fin_cellType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f532e7-fef5-464e-b3b6-27b94aa90fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin: Smooth muscle\n"
     ]
    }
   ],
   "source": [
    "markers = \"TAGLN, ACTA2, MYL9, IGFBP5, MCAM, CALD1, TPM2, NOTCH3, IGFBP7, MAP1B\"\n",
    "\n",
    "print(\"Fin: \"+ generate_response(pipe, g_tokenizer, pipe2, s_tokenizer, pipe3, e_tokenizer, markers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df595088-cf80-448f-9d2a-3a5f87035fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f145ed9-25c2-450d-99d7-b103d7b23046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "    count = 0\n",
    "    test_list = []\n",
    "    for item in data:\n",
    "        print(count)\n",
    "        if count == 50:\n",
    "            break\n",
    "        res = generate_response(pipe, g_tokenizer, pipe2, s_tokenizer, pipe3, e_tokenizer, item['Instruction'])\n",
    "        test_list.append({item['Output'] : res})\n",
    "        count += 1\n",
    "\n",
    "    with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "53ab9ab5-9d1e-49d3-a788-70bbcfb6f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe_base = pipeline(\n",
    "          \"text-generation\",\n",
    "          model=g_model,\n",
    "          tokenizer=g_tokenizer,\n",
    "          max_new_tokens=500,\n",
    "          return_full_text=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177efaa-c452-409a-bb40-06fb0b156d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"test_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "    count = 0\n",
    "    test_list = []\n",
    "    for item in data:\n",
    "        print(count)\n",
    "        if count == 50:\n",
    "            break\n",
    "\n",
    "        prompt_test = f\"\"\" Given the expression of genes {item['Instruction']}, identify the most likely cell type.\n",
    "\n",
    "        Answer: \n",
    "        \"\"\"\n",
    "        res = pipe_base(prompt_test, temperature=0.9, return_full_text=False)[0]['generated_text']\n",
    "        res = res[len(prompt_test):].strip()\n",
    "        test_list.append({item['Output'] : res})\n",
    "        count += 1\n",
    "\n",
    "    with open(\"output2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_list, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
