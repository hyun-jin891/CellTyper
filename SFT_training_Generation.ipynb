{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938c5ef-96c7-41ab-bd02-42ae5589c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c055d0-c638-4e2e-b01d-32516c6efd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1e702-07c8-4e8e-a906-b247c66b1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863adf26-4d9b-4ebd-ba20-17925656e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f6f947-155d-453d-8c32-eb2f9b474269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ec726-52c1-4b8e-8a15-486114e6b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38608926-6af1-4703-aba1-8093536010f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc039bbc-8005-4d09-b31d-3cf38b0a4b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6045/2306388850.py:1: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, is_bfloat16_supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887d26c-55a4-4baa-b306-b83bf5941920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,         \n",
    "    load_in_4bit = True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4831928-3e26-47ac-962b-d85184d314dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.6.2 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbfe0e-aeb5-4070-af97-dd178a9d4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"generation_SFTDataset.csv\", encoding=\"latin1\")\n",
    "\n",
    "df.to_csv(\"generation_SFTDataset.csv\",\n",
    "          index=False,\n",
    "          encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ed7f9-baad-45bf-9c0c-8faaddeeb236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb86173fce48f9a358ec8526d467f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"generation_SFTDataset.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fcdfc63-86c2-43e6-b1f2-179dc4caf043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acinar cell\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537fbae7-4713-4027-833e-c7173ae08700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'reasoning', 'output'],\n",
      "        num_rows: 9600\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adfdbd5b-aa0f-4326-b48c-3af018241ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "input_prompt = \"You are an intelligent expert to identify the most appropriate cell type from the given marker genes.\\nYou will be given marker genes and context that you should consider.\\n\\n**Do NOT explain your answer. You must return a single cell type word.**\\n\\n{}\\n\\nContext:\\n\\n{}\\n\\nAnswer: {}\"\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"reasoning\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, ainput, output in zip(instructions, inputs, outputs):\n",
    "        text = input_prompt.format(instruction, ainput, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5cb05e5-02e5-433d-82fe-c9d42820c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e4d580-7e43-4984-8d6b-f300476e8f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82473daa57b4990aa47ca1ca93c7c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3fd589b-ed74-476d-b981-1e0171eb32e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent expert to identify the most appropriate cell type from the given marker genes.\n",
      "You will be given marker genes and context that you should consider.\n",
      "\n",
      "**Do NOT explain your answer. You must return a single cell type word.**\n",
      "\n",
      "Given the expression of genes ERP27, PTF1A, RBPJL, PRSS1, GRP78, identify the most likely cell type.\n",
      "\n",
      "Context:\n",
      "\n",
      "Trypsinogen (PRSS1/2) is a zymogen secreted by acinar cells for protein digestion. PTF1A is a master transcription factor governing acinar cell identity and digestive enzyme gene expression. These gene expression patterns strongly indicate an acinar cell identity.\n",
      "\n",
      "Answer: Acinar cell<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f1adf9-4832-4bc1-8818-2e876aba33b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac3bfd453d94a5a9c36a1387ad1e03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./outputs\"\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=12,\n",
    "    packing=True,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=2.0,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=50,\n",
    "        logging_first_step=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.02,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=0.3,\n",
    "        dataloader_num_workers=8,\n",
    "        dataloader_pin_memory=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f48221-e494-482f-b48d-3b18b5c577ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,600 | Num Epochs = 2 | Total steps = 600\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 18,350,080/3,000,000,000 (0.61% trained)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 46:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.900300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.650200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=1.0142843445142111, metrics={'train_runtime': 2807.7358, 'train_samples_per_second': 6.838, 'train_steps_per_second': 0.214, 'total_flos': 6.15856351668265e+16, 'train_loss': 1.0142843445142111})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb8c5c11-5e03-4ac9-b88f-00945cfe5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_adapter_path = \"./outputs/sft_lora_adapter_generation\"\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(\n",
    "        sft_adapter_path,\n",
    "        save_adapter=True,\n",
    "        save_config=True\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to save model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7f485-ccc4-45d7-a660-43cb0b6db10f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
